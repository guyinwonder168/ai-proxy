package gemini

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"sync"
	"time"

	"ai-proxy/internal/bufferpool"
	"ai-proxy/internal/client"
	"ai-proxy/internal/errors"
	"ai-proxy/internal/schema"
)

// pooledReaderReadCloser wraps a bytes.Reader to implement io.ReadCloser
// and returns the reader to the pool when closed
type pooledReaderReadCloser struct {
	reader *bytes.Reader
}

type Throttler struct {
	lastTimeRequest time.Time
	mutex           sync.Mutex
	maxTimeoutTime  time.Duration
}

func NewThrottler(maxTimeout time.Duration) *Throttler {
	return &Throttler{
		lastTimeRequest: time.Now(),
		maxTimeoutTime:  maxTimeout,
	}
}

// Read implements the io.Reader interface
func (p *pooledReaderReadCloser) Read(b []byte) (int, error) {
	return p.reader.Read(b)
}

// Close implements the io.Closer interface and returns the reader to the pool
func (p *pooledReaderReadCloser) Close() error {
	bufferpool.PutReader(p.reader)
	return nil
}

func (t *Throttler) Wait(modelName string) {
	t.mutex.Lock()
	defer t.mutex.Unlock()

	if !time.Now().After(t.lastTimeRequest.Add(t.maxTimeoutTime)) {
		waitTime := time.Until(t.lastTimeRequest.Add(t.maxTimeoutTime))
		log.Printf("Throttled %v sec for %s", waitTime.Seconds(), modelName)
		time.Sleep(waitTime)
	}

	t.lastTimeRequest = time.Now()
}

type RequestAutoGenerated struct {
	Contents []Contents `json:"contents,omitempty"`
	// SystemInstruction Contents   `json:"systemInstruction,omitempty"`
}

type Parts struct {
	Text string `json:"text,omitempty"`
}

type Contents struct {
	Role  string  `json:"role,omitempty"`
	Parts []Parts `json:"parts,omitempty"`
}

type SystemInstruction struct {
	Contents []Contents `json:"contents,omitempty"`
}

type ResponceGenerated struct {
	Candidates []struct {
		Content struct {
			Parts []struct {
				Text string `json:"text,omitempty"`
			} `json:"parts,omitempty"`
			Role string `json:"role,omitempty"`
		} `json:"content,omitempty"`
		FinishReason string `json:"finishReason,omitempty"`
		Index        int    `json:"index,omitempty"`
	} `json:"candidates,omitempty"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount,omitempty"`
		CandidatesTokenCount int `json:"candidatesTokenCount,omitempty"`
		TotalTokenCount      int `json:"totalTokenCount,omitempty"`
	} `json:"usageMetadata,omitempty"`
	Error struct {
		Code    int    `json:"code,omitempty"`
		Message string `json:"message,omitempty"`
		Status  string `json:"status,omitempty"`
	} `json:"error,omitempty"`
}

func CreateRequest(providerURL, model, token string, reqBody schema.RequestOpenAICompatable) (*http.Request, error) {
	var body RequestAutoGenerated

	for _, v := range reqBody.Messages {
		switch v.Role {
		// case "system":
		// 	body.SystemInstruction = Contents{
		// 		Role:  "user",
		// 		Parts: []Parts{{Text: v.Content}},
		// 	}
		case "system":
			body.Contents = append(body.Contents, Contents{
				Role:  "user",
				Parts: []Parts{{Text: v.Content}},
			})
		case "user":
			body.Contents = append(body.Contents, Contents{
				Role:  v.Role,
				Parts: []Parts{{Text: v.Content}},
			})
		case "assistant":
			body.Contents = append(body.Contents, Contents{
				Role:  "model",
				Parts: []Parts{{Text: v.Content}},
			})
		default:
			body.Contents = append(body.Contents, Contents{
				Role:  v.Role,
				Parts: []Parts{{Text: v.Content}},
			})
		}
	}

	jsonBody, err := json.Marshal(body)
	if err != nil {
		return nil, err
	}

	// fmt.Printf("JSON BODY: %s\n", string(jsonBody))

	// Use a pooled reader instead of bytes.NewReader
	reader := bufferpool.GetReader()
	reader.Reset(jsonBody)

	req, err := http.NewRequest(http.MethodPost, providerURL, reader)
	if err != nil {
		// Return the reader to the pool if there's an error
		bufferpool.PutReader(reader)
		return nil, err
	}

	// Add a function to return the reader to the pool after the request is done
	// We can do this by replacing the request body with a custom ReadCloser
	req.Body = &pooledReaderReadCloser{reader: reader}

	return req, nil
}

func (c *GeminiClient) callWithClient(reqBody []byte) ([]byte, error) {
	var requestBody schema.RequestOpenAICompatable

	err := json.Unmarshal(reqBody, &requestBody)
	if err != nil {
		return nil, errors.NewValidationError(fmt.Sprintf("error in json.Unmarshal: %v", err))
	}

	// Execute requests with throttling
	c.throttler.Wait(c.Model)

	req, err := CreateRequest(c.ProviderURL, c.Model, c.Token, requestBody)
	if err != nil {
		return nil, err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return nil, err
	}

	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	if resp.StatusCode != http.StatusOK {
		return body, errors.NewProviderError(fmt.Sprintf("unexpected status code: %d", resp.StatusCode), "Google", c.Model, resp.StatusCode, nil)
	}

	var response ResponceGenerated

	err = json.Unmarshal(body, &response)
	if err != nil {
		return nil, err
	}

	return transformResponse(response, c.Model)
}

func transformResponse(response ResponceGenerated, modelName string) ([]byte, error) {
	if response.Error.Message != "" {
		return nil, errors.NewProviderError(fmt.Sprintf("error code %d - %s", response.Error.Code, response.Error.Message), "Google", modelName, response.Error.Code, nil)
	}

	if len(response.Candidates) == 0 {
		return nil, errors.NewProviderError("no candidates", "Google", modelName, 500, nil)
	}

	type Message struct {
		Role    string `json:"role"`
		Content string `json:"content"`
	}

	type Choices struct {
		Index        int     `json:"index"`
		Message      Message `json:"message"`
		FinishReason string  `json:"finish_reason"`
		Logprobs     any     `json:"logprobs"`
	}

	type Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	}

	type ContentResponse struct {
		ID      string    `json:"id"`
		Model   string    `json:"model,omitempty"`
		Object  string    `json:"object"`
		Created int       `json:"created"`
		Choices []Choices `json:"choices"`
		Usage   Usage     `json:"usage"`
	}

	var respTxt ContentResponse

	for _, v := range response.Candidates {
		var ch Choices

		ch.Index = v.Index
		if v.Content.Role == "model" {
			ch.Message.Role = "assistant"
		} else {
			ch.Message.Role = v.Content.Role
		}

		ch.Message.Content = v.Content.Parts[0].Text
		ch.FinishReason = v.FinishReason

		respTxt.Choices = append(respTxt.Choices, ch)
	}

	respTxt.Usage.PromptTokens = response.UsageMetadata.PromptTokenCount
	respTxt.Usage.CompletionTokens = response.UsageMetadata.CandidatesTokenCount
	respTxt.Usage.TotalTokens = response.UsageMetadata.TotalTokenCount

	return json.Marshal(respTxt)
}

// GeminiClient implements the Provider interface for Google Gemini
type GeminiClient struct {
	ProviderURL string
	Model       string
	Token       string
	httpClient  *http.Client
	throttler   *Throttler
}

// NewGeminiClient creates a new GeminiClient with the provided configuration
func NewGeminiClient(providerURL, model, token string, config schema.HttpClientConfig) *GeminiClient {
	return &GeminiClient{
		ProviderURL: providerURL,
		Model:       model,
		Token:       token,
		httpClient:  client.NewHttpClient(config),
		throttler:   NewThrottler(10 * time.Second),
	}
}

// Call implements the Provider interface
func (c *GeminiClient) Call(ctx context.Context, payload []byte, path string) ([]byte, error) {
	return c.callWithClient(payload)
}
