[![Bugs](https://sonarcloud.io/api/project_badges/measure?project=evgensoft_ai-proxy&metric=bugs)](https://sonarcloud.io/summary/new_code?id=evgensoft_ai-proxy)
[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=evgensoft_ai-proxy&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=evgensoft_ai-proxy)

# Proxy for LLM Providers

## Overview
This service acts as a proxy for various Large Language Model (LLM) providers, including OpenAI, Groq, HuggingFace, and others. It allows users to seamlessly interact with multiple LLMs through a unified API, simplifying integration and request management with secure proxy access token authentication.

## Features
- **Unified API**: Work with multiple LLM providers using a single API endpoint.
- **Provider Flexibility**: Easily switch between different LLM providers without changing your application code.
- **Secure Authentication**: Proxy access token authentication for API security.
- **Request Management**: Handles routing and error handling.
- **Rate Limiting**: Supports per-model request limits (minute/hour/day).
- **Simple Configuration**: YAML-based setup with support for multiple models.

## Getting Started

### Prerequisites
- Go (version 1.24 or higher)

### Installation

Clone the repository:
```bash
git clone https://github.com/evgensoft/ai-proxy.git
cd ai-proxy
```

### Configuration

- `.env` - Proxy settings (proxy access token, port)
  - Copy from `.env.example`
  - Set `AUTH_TOKEN` to a secure proxy access token for authenticating requests to this proxy
- `provider_config.yaml` - Provider-specific configurations
  - Copy from `provider_config-example.yaml`
  - Add API keys for each provider

> ‚úÖ You can list multiple models from different providers in the same file.
> üõ°Ô∏è Sensitive values like API tokens should be stored securely.

```yaml
models:
  - name: groq/llama-3.2-90b-vision-preview
    provider: groq
    priority: 1
    requests_per_minute: 10
    requests_per_hour: 100
    requests_per_day: 3500
    url: "https://api.groq.com/openai/v1/chat/completions"
    token: "your_groq_api_token"
    max_request_length: 128000
    model_size: BIG
```

> ‚úÖ You can list multiple models from different providers in the same file.
> üõ°Ô∏è Sensitive values like API tokens should be stored securely.

### Quick Start

1. Copy configuration templates:
```bash
cp .env.example .env
cp provider_config-example.yaml provider_config.yaml
```

2. Edit `.env` and set:
```bash
AUTH_TOKEN="your_token_here"  # REQUIRED
PORT="8080"                   # Optional (default: 8080)
```

3. Edit `provider_config.yaml` with your provider API keys

4. Build and run:
```bash
go build -o ai-proxy && ./ai-proxy
```

### Running the Service

To start the proxy server, run:

```bash
go run main.go
```
The server will start on `http://localhost:8080` by default. You can change the port by setting the `PORT` environment variable:
```bash
PORT=9090 go run main.go
```
Replace `9090` with your desired port number.

## Example Usage

To use the proxy, you need to include the proxy access token in the Authorization header of your requests:

### Using cURL

```bash
curl -X POST http://localhost:8080/chat/completions \
     -H "Authorization: Bearer ${AUTH_TOKEN}" \
     -H "Content-Type: application/json" \
     -d '{
           "model": "groq/llama-3.2-90b-vision-preview",
           "messages": [
             {
               "role": "system",
               "content": "You are a helpful assistant."
             },
             {
               "role": "user",
               "content": "Tell me a joke."
             }
           ]
         }'
```

Replace `${AUTH_TOKEN}` with your actual proxy access token.
## Streaming API

The proxy now supports streaming responses for providers that implement this feature. To use streaming, clients must send `"stream": true` in the JSON body of their `POST` request to `/chat/completions`. You also need to include the proxy access token in the Authorization header.

Here's an example of a streaming request using `curl`:

```bash
curl -N -X POST http://localhost:8080/chat/completions \
-H "Authorization: Bearer ${AUTH_TOKEN}" \
-H "Content-Type: application/json" \
-d '{
  "model": "openrouter/google/gemini-flash-1.5",
  "messages": [
    {
      "role": "user",
      "content": "Tell me a very long story."
    }
  ],
  "stream": true
}'
```

Replace `${AUTH_TOKEN}` with your actual proxy access token.

When streaming is enabled, the response will be a `text/event-stream` containing server-sent events (SSE). Each event will contain a chunk of the response as it's generated by the model.

Note: Currently, streaming is only implemented for the OpenRouter provider, but support will be expanded to other providers in future updates.

## Contributing
Contributions are welcome! Please submit a pull request or open an issue to discuss improvements.

## License
## Authentication

The API requires a valid proxy access token set in the `Authorization` header.

1. Set your proxy access token in the environment:
```bash
export AUTH_TOKEN="your_proxy_access_token_here"
```

2. Include in requests:
```bash
curl -H "Authorization: Bearer ${AUTH_TOKEN}" ...
```

### Example Usage with Authentication

To use the proxy, you need to include the proxy access token in the Authorization header of your requests:

```bash
curl -X POST http://localhost:8080/chat/completions \
     -H "Authorization: Bearer ${AUTH_TOKEN}" \
     -H "Content-Type: application/json" \
     -d '{
           "model": "groq/llama-3.2-90b-vision-preview",
           "messages": [
             {
               "role": "system",
               "content": "You are a helpful assistant."
             },
             {
               "role": "user",
               "content": "Tell me a joke."
             }
           ]
         }'
```

Replace `${AUTH_TOKEN}` with your actual proxy access token.
This project is licensed under the MIT License.